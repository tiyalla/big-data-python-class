{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Create a crawler using the webcrawler provided and crawl 200 pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 Install Scrapy, a framework for crawling websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (4.2.5)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: Twisted>=13.1.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (18.7.0)\n",
      "Requirement already satisfied: pyOpenSSL in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (18.0.0)\n",
      "Requirement already satisfied: queuelib in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: cssselect>=0.9 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (1.0.3)\n",
      "Requirement already satisfied: parsel>=1.1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (1.5.1)\n",
      "Requirement already satisfied: service-identity in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (17.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (1.11.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from scrapy) (1.19.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (4.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (0.7.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (18.0.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (1.9.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (18.2.0)\n",
      "Requirement already satisfied: cryptography>=2.2.1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from pyOpenSSL->scrapy) (2.3.1)\n",
      "Requirement already satisfied: functools32; python_version < \"3.0\" in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from parsel>=1.1->scrapy) (3.2.3.post2)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from service-identity->scrapy) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from service-identity->scrapy) (0.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy) (40.2.0)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->scrapy) (2.7)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (0.24.0)\n",
      "Requirement already satisfied: enum34; python_version < \"3\" in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.1.6)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.11.5)\n",
      "Requirement already satisfied: ipaddress; python_version < \"3\" in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.0.22)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tonye iyalla\\anaconda2\\lib\\site-packages (from cffi!=1.11.3,>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy) (2.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grin 1.2.1 requires argparse>=1.1, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scrapy project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'tutorial', using template directory 'c:\\\\users\\\\tonye iyalla\\\\anaconda2\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\n",
      "    C:\\Users\\Tonye Iyalla\\Documents\\BigData\\big-data-python-class\\Homeworks\\Homework7\\tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Local Disk\n",
      " Volume Serial Number is FE3C-B042\n",
      "\n",
      " Directory of C:\\Users\\Tonye Iyalla\\Documents\\BigData\\big-data-python-class\\Homeworks\\Homework7\n",
      "\n",
      "11/12/2018  07:36 PM    <DIR>          .\n",
      "11/12/2018  07:36 PM    <DIR>          ..\n",
      "11/11/2018  02:01 PM    <DIR>          .ipynb_checkpoints\n",
      "11/12/2018  07:16 PM               867 amazon.py\n",
      "11/12/2018  07:31 PM            12,811 Iyalla-HW7-linkanalysis-partFall2018.ipynb\n",
      "11/12/2018  07:36 PM    <DIR>          tutorial\n",
      "11/11/2018  02:40 PM             7,019 Untitled.ipynb\n",
      "               3 File(s)         20,697 bytes\n",
      "               4 Dir(s)  201,833,095,168 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd into tutorial spiders directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tonye Iyalla\\Documents\\BigData\\big-data-python-class\\Homeworks\\Homework7\\tutorial\\tutorial\\spiders\n"
     ]
    }
   ],
   "source": [
    "cd tutorial/tutorial/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider 'amazon' already exists in module:\n",
      "  tutorial.spiders.amazon\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider amazon amazon.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Create crawler and crawl webpage --amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load amazon.py\n",
    "import scrapy\n",
    "\n",
    "class amazonItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    resp = scrapy.Field()\n",
    "class AmazonSpider(scrapy.Spider):\n",
    "    name = 'amaz'\n",
    "    allowed_domains = ['https://www.amazon.com/']\n",
    "    start_urls = ['https://www.amazon.com/books-used-books-textbooks/b/?ie=UTF8&node=283155&ref_=topnav_storetab_b','https://www.amazon.com/gp/browse.html?node=2625373011&ref_=nav_em_T1_0_4_14_1__mov', 'https://www.amazon.com/b/ref=s9_acss_bw_cg_TXTHPCCG_1c1_w?node=468204&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-8&pf_rd_r=F3C0AEBDPSV192J7BNP8&pf_rd_t=101&pf_rd_p=b48a6433-fbec-472a-9c4f-db43997d01de&pf_rd_i=465600']\n",
    "\n",
    "    def parse(self, response):\n",
    "        res = scrapy.Selector(response)\n",
    "        titles = res.xpath('//ul/li')\n",
    "        items = []\n",
    "        for title in titles:\n",
    "            item = nbaItem()\n",
    "            item[\"title\"] = title.xpath(\"a/text()\").extract()\n",
    "            item[\"link\"] = title.xpath(\"a/@href\").extract()\n",
    "            item[\"resp\"] = response\n",
    "            if item[\"title\"] != []:\n",
    "                items.append(item)\n",
    "\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl amaz -o amazon.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2. Stochastic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data, extra words surrounding the link e.g. response code 200 and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://www.amazon.com/movies-tv-dvd-bluray/b?ie=UTF8&node=2625373011>\n",
      "['https://www.amazon.com/movies-tv-dvd-bluray/b?ie=UTF8&node=2625373011']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "temp =[]\n",
    "data=pd.read_csv(\"amazon.csv\")\n",
    "data['link']='http://www.amazon.com'+data['link']\n",
    "x=data['resp'][0]\n",
    "print x\n",
    "temp.append((x.split()[-1]).split('>')[-2])\n",
    "print temp\n",
    "for i in range(len(data)):\n",
    "    if data['resp'][i] == x:\n",
    "        temp.append(data['link'][i])\n",
    "    else:\n",
    "        x=data['resp'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_result = list(pd.DataFrame(temp)[0].unique())\n",
    "links = []\n",
    "length = len(temp)\n",
    "for i, val in enumerate(temp):\n",
    "    if i < length-1:\n",
    "        links.append((temp[i], temp[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stochastic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "df = pd.DataFrame(index=amazon_result, columns=amazon_result)\n",
    "m = df.replace(np.NaN, 0)\n",
    "for i in links:\n",
    "    m.loc[i] = 1.0\n",
    "arr = np.array(m)\n",
    "arrSum = arr.sum(axis=1)\n",
    "res = arr/arrSum[:, np.newaxis]\n",
    "matrixAr = np.nan_to_num(res)\n",
    "matrixAr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3. Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    " \n",
    "def pagerank(H):\n",
    "    n= len(H)\n",
    "    w = zeros(n)\n",
    "    rho = 1./n * ones(n);\n",
    "    for i in range(n):\n",
    "        if multiply.reduce(H[i]== zeros(n)):\n",
    "            w[i] = 1\n",
    "    newH = H + outer((1./n * w),ones(n))\n",
    " \n",
    "    theta=0.85\n",
    "    G = (theta * newH) + ((1-theta) * outer(1./n * ones(n), ones(n)))\n",
    "    print rho\n",
    "    for j in range(10):\n",
    "        rho = dot(rho,G)\n",
    "        print rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0L, 1L, 2L, 72L, 3L]\n"
     ]
    }
   ],
   "source": [
    "RankVal=pd.DataFrame(pageRank(matrixAr,s=.86))\n",
    "RankSort = (RankVal.sort_values(0)).head(5)\n",
    "RankList = list(RankSort.index)\n",
    "print RankList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 page URLs:\n",
      "https://www.amazon.com/movies-tv-dvd-bluray/b?ie=UTF8&node=2625373011\n",
      "http://www.amazon.com/Movies/b?ie=UTF8&node=2649512011\n",
      "http://www.amazon.com/tv-dvd-bluray/b?ie=UTF8&node=2649513011\n",
      "http://www.amazon.com/gp/promotions/details/popup/AWT354OR7BM1U/?ie=UTF8&ref=MoviesHPBB_POPG\n",
      "http://www.amazon.com/amazon-fire-tv-4k-uhd-streaming-media-player/dp/B01N32NCPM\n"
     ]
    }
   ],
   "source": [
    "urls = pd.DataFrame(amazon_result)\n",
    "print \"Top 5 page URLs:\"\n",
    "for i in RankList:\n",
    "    print urls[0][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4. Page Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits(A):\n",
    "    n= len(A)\n",
    "    Au= dot(transpose(A),A)\n",
    "    Hu = dot(A,transpose(A))\n",
    "    a = ones(n);\n",
    "    h = ones(n)\n",
    "    #print a,h\n",
    "    for j in range(5):\n",
    "        a = dot(a,Au)\n",
    "        a= a/sum(a)\n",
    "        h = dot(h,Hu)\n",
    "        h = h/ sum(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitVal = pd.DataFrame(hits(matrixAr))\n",
    "hitSort = (hitVal.sort_values(0, ascending=False)).head(5)\n",
    "hitList = list(hitSort.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 page URLs:\n",
      "http://www.amazon.com/b?ie=UTF8&node=11913537011\n",
      "http://www.amazon.comhttps://www.amazon.com/l/ref=map_1_b2b_GW_FT?node=17882322011\n",
      "http://www.amazon.com/Art-House-International-DVD/b?ie=UTF8&node=163313\n",
      "http://www.amazon.com/Accessories-Books/b?ie=UTF8&node=265040\n",
      "http://www.amazon.com/iss/credit/rewardscardmember?_encoding=UTF8&plattr=CBFOOT\n"
     ]
    }
   ],
   "source": [
    "urls = pd.DataFrame(amazon_result)\n",
    "print \"Top 5 page URLs:\"\n",
    "for i in hitList:\n",
    "    print urls[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
